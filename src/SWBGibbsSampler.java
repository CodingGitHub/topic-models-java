/*
 * SWBGibbsSampler is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the Free
 * Software Foundation; either version 2 of the License, or (at your option) any
 * later version.
 *
 * CiLdaGibbsSampler is distributed in the hope that it will be useful, but
 * WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 * FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
 * details.
 *
 * You should have received a copy of the GNU General Public License along with
 * this program; if not, write to the Free Software Foundation, Inc., 59 Temple
 * Place, Suite 330, Boston, MA 02111-1307 USA
 */

/*
 * Created on Feb 3, 2015
 *
 * 	Imran SHEIKH
 *	imran.sheikh@loria.fr, mranahmd@gmail.com
 * 	Multispeech, LORIA-INRIA
 * 	Nancy, France
 *
 *  Acknowledgement: Gregor Heinrich (gregor :: arbylon : net) 
 *  (This file is adapted from the Java code of Gregor Heinrich (gregor@arbylon.net)
 *    http://www.arbylon.net/projects/LdaGibbsSampler.java
 *  part of the org.knowceans experimental software packages.)
 *
 */

import java.text.DecimalFormat;
import java.text.NumberFormat;

import java.io.*;

/**
 * Gibbs sampler for estimating the best assignments of words to topics, 
 * documents, corpus. The algorithm is introduced in C Chemudugunta s' paper
 * "Modeling General and Specific Aspects of Documents with a Probabilistic Topic Model"
 * (2007).
 * 
 * @author mranahmd
 */
public class SWBGibbsSampler {

    /**
     * document data (term lists)
     */
    int[][] documents;

    /**
     * vocabulary size
     */
    int V;

    /**
     * number of topics
     */
    int K;

    /**
     * Dirichlet parameter (document--topic associations)
     */
    double alpha;

    /**
     * Dirichlet parameter (topic--term associations)
     */
    double beta;

    /**
     * topic assignments for each word.
     */
    int z[][];

    /**
     * cwt[i][j] number of instances of word i (term?) assigned to topic j.
     */
    int[][] nw;

    /**
     * na[i][j] number of words in document i assigned to topic j.
     */
    int[][] nd;

    /**
     * nwsum[j] total number of words assigned to topic j.
     */
    int[] nwsum;

    /**
     * nasum[i] total number of words in document i.
     */
    int[] ndsum;

	/**
	* number of times ith document generated jth word
	*/	    
    int[][] ndDoc;

	/**
	* number of times ith word generated by background/corpus
	*/   
    int[] ndBg;

	/**
	* corpus/background generated words in entire corpus
	*/    
    int sumBg;
   
	/**
	* number of words generated by corpus/background, document, topic in each document
	*/  
    int[][] ndgen;

	/**
	* what generates jth word in ith document? corpus or document or topic
	*/    
    int[][] genA;

	double betaD;
	
	double betaBg;

	double gamma;

    /**
     * cumulative statistics of theta
     */
    double[][] thetasum;

    /**
     * cumulative statistics of phi
     */
    double[][] phisum;

    /**
     * size of statistics
     */
    int numstats;

    /**
     * sampling lag (?)
     */
    private static int THIN_INTERVAL = 20;

    /**
     * burn-in period
     */
    private static int BURN_IN = 100;

    /**
     * max iterations
     */
    private static int ITERATIONS = 1000;

    /**
     * sample lag (if -1 only one sample taken)
     */
    private static int SAMPLE_LAG;

    private static int dispcol = 0;

    /**
     * Initialise the Gibbs sampler with data.
     * 
     * @param V
     *            vocabulary size
     * @param data
     */
    public SWBGibbsSampler(int[][] documents, int V) {

        this.documents = documents;
        this.V = V;
    }

    /**
     * Initialisation: Must start with an assignment of observations to topics 
     * 
     * @param K
     *            number of topics
     * @return z assignment of topics to words
     */
    public void initialState(int K) {
        int i;

        int M = documents.length;

        // initialise count variables.
        nw = new int[V][K];
        nd = new int[M][K];
        nwsum = new int[K];
        ndsum = new int[M];

        ndgen = new int[M][3];
       
        ndDoc = new int[M][V];
        ndBg = new int[V];
	sumBg = 0;

        // The z_i are are initialised to values in [1,K] to determine the
        // initial state of the Markov chain.
        z = new int[M][];
        genA = new int[M][];
        for (int m = 0; m < M; m++) {
            int N = documents[m].length;
            z[m] = new int[N];
            genA[m] = new int[N];
            for (int n = 0; n < N; n++) {
            	int gen = (int) (Math.random() * 3);
            	genA[m][n] = gen;
            	if(gen == 0){		// topic generated word
                	int topic = (int) (Math.random() * K);
                	z[m][n] = topic;
                	// number of instances of word i assigned to topic j
                	nw[documents[m][n]][topic]++;
                	// number of words in document i assigned to topic j.
                	nd[m][topic]++;
                	// total number of words assigned to topic j.
                	nwsum[topic]++;
            		// total number of words in document i generated by topics
            		ndsum[m]++;
                }
                if(gen == 1){		// document generated word
                	z[m][n] = -1;
                	ndDoc[m][documents[m][n]]++;
                }
                if(gen == 2){		// background generated word
                	z[m][n] = -1;
                	ndBg[documents[m][n]]++;
			sumBg++;
                }
                ndgen[m][gen]++;
            }
        }
    }

    /**
     * Main method: Select initial state ? Repeat a large number of times: 1.
     * Select an element 2. Update conditional on other elements. If
     * appropriate, output summary for each run.
     * 
     * @param K
     *            number of topics
     * @param alpha
     *            symmetric prior parameter on document--topic associations
     * @param beta
     *            symmetric prior parameter on topic--term associations
     */
    private void gibbs(int K, double alpha, double beta, double betaDoc, double betaBg, double gamma ) {
        this.K = K;
        this.alpha = alpha;
        this.beta = beta;
        this.betaD = betaDoc;
        this.betaBg = betaBg;
        this.gamma = gamma;

        // init sampler statistics
        if (SAMPLE_LAG > 0) {
            thetasum = new double[documents.length][K];
            phisum = new double[K][V];
            numstats = 0;
        }

        // initial state of the Markov chain:
        initialState(K);

        System.out.println("Sampling " + ITERATIONS
            + " iterations with burn-in of " + BURN_IN + " (B/S="
            + THIN_INTERVAL + ").");

        for (int i = 0; i < ITERATIONS; i++) {

            // for all z_i
            for (int m = 0; m < z.length; m++) {
                for (int n = 0; n < z[m].length; n++) {

                    // (z_i = z[m][n])
                    // sample from p(z_i|z_-i, w)
                    sampleFullConditional(m, n);

                }
            }

            if ((i < BURN_IN) && (i % THIN_INTERVAL == 0)) {
                System.out.print("B");
                dispcol++;
            }
            // display progress
            if ((i > BURN_IN) && (i % THIN_INTERVAL == 0)) {
                System.out.print("S");
                dispcol++;
            }
            // get statistics after burn-in
            if ((i > BURN_IN) && (SAMPLE_LAG > 0) && (i % SAMPLE_LAG == 0)) {
                updateParams();
                System.out.print("|");
                if (i % THIN_INTERVAL != 0)
                    dispcol++;
            }
            if (dispcol >= 100) {
                System.out.println();
                dispcol = 0;
            }
        }
    }

    /**
     * Sample a topic and word assignment from the full conditional distribution: 
     * 
     * @param m
     *            document
     * @param n
     *            word
     */
    private void sampleFullConditional(int m, int n) {

		if(genA[m][n] == 0){		// topic generated word
        	// remove z_i from the count variables
        	int topic = z[m][n];
        	nw[documents[m][n]][topic]--;
        	nd[m][topic]--;
        	nwsum[topic]--;
	        ndsum[m]--;
        }
        if(genA[m][n] == 1){		// document generated word
        	ndDoc[m][documents[m][n]]--;
        }
        if(genA[m][n] == 2){		// background generated word
        	ndBg[documents[m][n]]--;
		sumBg--;
        }
		ndgen[m][genA[m][n]]--;

       	// do multinomial sampling via cumulative method:
        double[] p = new double[K + 2];
        for (int k = 0; k < K; k++) {
        	p[k] = (ndgen[m][0] + gamma) / (documents[m].length -1 + 3 * gamma)
        		* (nw[documents[m][n]][k] + beta) / (nwsum[k] + V * beta)
        	        * (nd[m][k] + alpha) / (ndsum[m] + K * alpha);
        }        
    
    	p[K] = (ndgen[m][1] + gamma) / (documents[m].length -1 + 3 * gamma)
    		* (ndDoc[m][documents[m][n]] + betaD) / (ndgen[m][1] + V * betaD);
    		
    	p[K+1] = (ndgen[m][2] + gamma) / (documents[m].length -1 + 3 * gamma)
    		* (ndBg[documents[m][n]] + betaBg) / (sumBg + V * betaBg);

        // cumulate multinomial parameters
        for (int k = 1; k < p.length; k++) {
            p[k] += p[k - 1];
        }
        // scaled sample because of unnormalised p[]
        double u = Math.random() * p[K + 1];
        int g = 0;
        for (g = 0; g < p.length; g++) {
            if (u < p[g])
                break;
        }

		if(g < K){
	        // add newly estimated z_i to count variables
    	    nw[documents[m][n]][g]++;
        	nd[m][g]++;
	        nwsum[g]++;
	        z[m][n] = g;
	    	genA[m][n] = 0;
	    	ndgen[m][0]++;
	    	ndsum[m]++;
    	}
    	if(g == K){
        	ndDoc[m][documents[m][n]]++;
        	z[m][n] = -1;
	    	genA[m][n] = 1;
			ndgen[m][1]++;
    	}
    	if(g == K+1){
        	ndBg[documents[m][n]]++;
			sumBg++;
        	z[m][n] = -1;
	    	genA[m][n] = 2;
	    	ndgen[m][2]++;
    	}
    }

    /**
     * Add to the statistics the values of theta and phi for the current state.
     */
    private void updateParams() {
        for (int m = 0; m < documents.length; m++) {
            for (int k = 0; k < K; k++) {
                thetasum[m][k] += (nd[m][k] + alpha) / (ndsum[m] + K * alpha);
            }
        }
        for (int k = 0; k < K; k++) {
            for (int w = 0; w < V; w++) {
                phisum[k][w] += (nw[w][k] + beta) / (nwsum[k] + V * beta);
            }
        }
        numstats++;
    }

    /**
     * Retrieve estimated document--topic associations. If sample lag > 0 then
     * the mean value of all sampled statistics for theta[][] is taken.
     * 
     * @return theta multinomial mixture of document topics (M x K)
     */
    public double[][] getTheta() {
        double[][] theta = new double[documents.length][K];
        for (int m = 0; m < documents.length; m++) {
                for (int k = 0; k < K; k++) {
                    theta[m][k] = (nd[m][k] + alpha) / (ndsum[m] + K * alpha);
                }
            }

        return theta;
    }

    /**
     * Retrieve estimated topic--word associations. If sample lag > 0 then the
     * mean value of all sampled statistics for phi[][] is taken.
     * 
     * @return phi multinomial mixture of topic words (K x V)
     */
    public double[][] getPhi() {
        double[][] phi = new double[K][V];
        for (int k = 0; k < K; k++) {
                for (int w = 0; w < V; w++) {
                    phi[k][w] = (nw[w][k] + beta) / (nwsum[k] + V * beta);
                }
        }
        return phi;
    }
    
    public double[][] getBgDist(){
    	double [][] ret = new double[1][V];
    	for(int w=0; w<V; w++){
    		ret[0][w] = (ndBg[w] + betaBg) / (sumBg + V * betaBg);
    	}
    	return ret;
    }
    
    public double[][] getDocDist(){
    	double [][] ret = new double[documents.length][V];
    	for(int d=0; d<documents.length; d++){
	    	int tot = 0;
	    	for(int w=0; w<V; w++){
			tot += ndDoc[d][w];
		}
    		for(int w=0; w<V; w++){
    			ret[d][w] = (ndDoc[d][w] + betaD) / (tot + V * betaD);
    		}
    	}
    	return ret;
    }
    
    public double[][] getGenDist(){
	    double [][] ret = new double[documents.length][3];
	    for(int d=0; d<documents.length; d++){
	    	for(int i=0; i<3; i++){
	    		ret[d][i] = (ndgen[d][i] + gamma) / (documents[d].length + 3 * gamma);
	    	}
	    }
    	return ret;
    }
    
     /**
     * 
     * @return cwt (K x V)

     */
    public int[][] getCwt() {
        return nw;
    }

    /**
     * Configure the gibbs sampler
     * 
     * @param iterations
     *            number of total iterations
     * @param burnIn
     *            number of burn-in iterations
     * @param thinInterval
     *            update statistics interval
     * @param sampleLag
     *            sample interval (-1 for just one sample at the end)
     */
    public void configure(int iterations, int burnIn, int thinInterval,
        int sampleLag) {
        ITERATIONS = iterations;
        BURN_IN = burnIn;
        THIN_INTERVAL = thinInterval;
        SAMPLE_LAG = sampleLag;
    }

    public static void main(String[] args) {
	if (args.length != 8)
        {
            System.err.println("\nUSAGE: java SWBGibbsSampler <documentTermIndexFile> <vocabFile> <K> <alpha> <beta> <betaDoc> <betaBg> <gamma>\n");
            System.exit(0);
        }
        
        try
    	{
			CorpusReader cr = new CorpusReader(args[0], args[1]);
			int V = cr.getV();
			int M = cr.getD();
			int[][] documents = cr.getDocumentTermIndex();
		
			int K = Integer.parseInt(args[2]);
			double alpha = Float.parseFloat(args[3]);
			double beta = Float.parseFloat(args[4]);
			double betaDoc = Float.parseFloat(args[5]);
			double betaBg = Float.parseFloat(args[6]);
			double gamma = Float.parseFloat(args[7]);

			System.out.println("Latent Dirichlet Allocation using Gibbs Sampling.");
			System.out.println("\nTraining " + K + " LDA topics for " + M + " documents with " + V + " words vocabulary ...\n");

			SWBGibbsSampler swb = new SWBGibbsSampler(documents, V);
			swb.configure(2500, 1000, 100, 10);
			swb.gibbs(K, alpha, beta, betaDoc, betaBg, gamma);

			double[][] theta = swb.getTheta();
			double[][] phi = swb.getPhi();
			int[][] cwt = swb.getCwt();
			double[][] genDist = swb.getGenDist();
			double[][] bgDist = swb.getBgDist();

			TopicUtils.saveCSV(theta,"./SWB_theta");
			TopicUtils.saveCSV(phi,"./SWB_phi");
			TopicUtils.saveCSV(cwt,"./SWB_cwt");
			TopicUtils.saveCSV(genDist,"./SWB_docGenDist");
			TopicUtils.saveCSV(bgDist,"./SWB_bgWgenDist");
			TopicUtils.saveTopTopicWords(phi, cr.getVocab(), "./SWB_top-topic-words", 50);
			TopicUtils.saveTopTopicWords(bgDist, cr.getVocab(), "./SWB_top-bg-words", 50);

			double[][] docDist = swb.getDocDist();
			TopicUtils.saveTopTopicWords(docDist, cr.getVocab(), "./SWB_top-doc-words", 10);
			TopicUtils.saveCSV(docDist,"./SWB_docWgenDist");

		}catch (IOException ex){
				System.err.println(ex);
           		System.err.println("\nUSAGE: java SWBGibbsSampler <documentTermIndexFile> <vocabFile> <K> <alpha> <beta> <betaDoc> <betaBg> <gamma>\n");
		}
    }
}
